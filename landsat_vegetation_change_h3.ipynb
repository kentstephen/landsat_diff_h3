{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Landsat EVI/NDVI Change Detection with H3 Aggregation\n",
    "\n",
    "Streaming approach for memory-efficient processing of large raster datasets.\n",
    "\n",
    "**Key design:**\n",
    "- Process one time period at a time → aggregate to H3 → discard raster\n",
    "- Controlled concurrency (default 2) - never holds many rasters in memory\n",
    "- DuckDB H3 extension for fast spatial aggregation\n",
    "- Arrow for zero-copy data handoff\n",
    "- Only final pandas DataFrame for small result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x10c9679e0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import concurrent.futures\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pyarrow as pa\n",
    "import duckdb\n",
    "import odc.stac\n",
    "import boto3\n",
    "from pystac_client import Client\n",
    "from pyproj import Transformer\n",
    "from dotenv import load_dotenv\n",
    "import h3\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Configure dask for optimal S3 fetching\n",
    "dask.config.set({\n",
    "    'array.slicing.split_large_chunks': False,  # Avoid warnings\n",
    "    'num_workers': 8,  # Parallel chunk fetches\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install H3 extension once (persists to disk)\n",
    "duckdb.sql(\"INSTALL h3 FROM community\")\n",
    "\n",
    "def get_con():\n",
    "    \"\"\"Create a new DuckDB connection with H3 loaded.\"\"\"\n",
    "    con = duckdb.connect()\n",
    "    con.sql(\"LOAD h3\")\n",
    "    return con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_resolution_for_h3(\n",
    "    h3_res: int, \n",
    "    native_resolution: int = 30,\n",
    "    pixels_per_hex_edge: int = 6\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Calculate appropriate Landsat resolution based on H3 resolution.\n",
    "    \n",
    "    Uses ~4-10 Landsat pixels per H3 hex edge for good sampling.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    h3_res : int\n",
    "        H3 resolution (0-15)\n",
    "    native_resolution : int\n",
    "        Native sensor resolution in meters (default 30 for Landsat)\n",
    "    pixels_per_hex_edge : int\n",
    "        Target pixels per hex edge (default 6)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Resolution in meters, rounded to native resolution multiple\n",
    "    \"\"\"\n",
    "    hex_edge_m = h3.average_hexagon_edge_length(h3_res, unit='m')\n",
    "    target_resolution = hex_edge_m / pixels_per_hex_edge\n",
    "    \n",
    "    resolution_rounded = max(\n",
    "        round(target_resolution / native_resolution) * native_resolution,\n",
    "        native_resolution\n",
    "    )\n",
    "    \n",
    "    return resolution_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_aws_access():\n",
    "    \"\"\"Configure AWS credentials for odc-stac access.\"\"\"\n",
    "    load_dotenv()\n",
    "    \n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "        aws_secret_access_key=os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "        region_name='us-west-2'\n",
    "    )\n",
    "    \n",
    "    odc.stac.configure_s3_access(\n",
    "        aws_session=session,\n",
    "        requester_pays=True\n",
    "    )\n",
    "    \n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample_items(items, items_per_group: int = 3, max_items: int = None):\n",
    "    \"\"\"Select clearest images stratified by month AND path/row.\"\"\"\n",
    "    from collections import defaultdict\n",
    "    import math\n",
    "    \n",
    "    groups = defaultdict(list)\n",
    "    for item in items:\n",
    "        dt = item.datetime\n",
    "        path = item.properties.get('landsat:wrs_path', 0)\n",
    "        row = item.properties.get('landsat:wrs_row', 0)\n",
    "        key = (dt.year, dt.month, path, row)\n",
    "        groups[key].append(item)\n",
    "    \n",
    "    for key in groups:\n",
    "        groups[key] = sorted(\n",
    "            groups[key], \n",
    "            key=lambda x: x.properties.get('eo:cloud_cover', 100)\n",
    "        )[:items_per_group]\n",
    "    \n",
    "    total_selected = sum(len(g) for g in groups.values())\n",
    "    \n",
    "    if max_items is None or total_selected <= max_items:\n",
    "        selected = []\n",
    "        for key in sorted(groups.keys()):\n",
    "            selected.extend(groups[key])\n",
    "        return selected\n",
    "    \n",
    "    keep_ratio = max_items / total_selected\n",
    "    selected = []\n",
    "    for key in sorted(groups.keys()):\n",
    "        group_items = groups[key]\n",
    "        keep_count = max(1, math.floor(len(group_items) * keep_ratio))\n",
    "        selected.extend(group_items[:keep_count])\n",
    "    \n",
    "    if len(selected) > max_items:\n",
    "        selected = sorted(selected, key=lambda x: x.properties.get('eo:cloud_cover', 100))[:max_items]\n",
    "    \n",
    "    return selected\n",
    "\n",
    "\n",
    "def process_single_period_to_h3(\n",
    "    time_of_interest: str,\n",
    "    bounds: List[float],\n",
    "    h3_res: int = 7,\n",
    "    red_band: str = \"red\",\n",
    "    blue_band: str = \"blue\",\n",
    "    nir_band: str = \"nir08\",\n",
    "    collection: str = \"landsat-c2-l2\",\n",
    "    cloud_threshold: int = 20,\n",
    "    evi: bool = True,\n",
    "    native_resolution: int = 30,\n",
    "    max_items: int = 50,\n",
    "    items_per_group: int = 2,\n",
    "    agg_func: str = \"mean\",\n",
    "    verbose: bool = False,\n",
    ") -> Optional[pa.Table]:\n",
    "    \"\"\"\n",
    "    Load ONE time period, aggregate to H3 immediately, return small result.\n",
    "    Optimized for parallel I/O and minimal memory.\n",
    "    \"\"\"\n",
    "    import time\n",
    "    \n",
    "    pixel_spacing = calculate_resolution_for_h3(h3_res, native_resolution)\n",
    "    year_tag = time_of_interest[:4]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  [{year_tag}] Resolution: {pixel_spacing}m\")\n",
    "    \n",
    "    configure_aws_access()\n",
    "    \n",
    "    catalog = Client.open(\"https://earth-search.aws.element84.com/v1\")\n",
    "    \n",
    "    items = catalog.search(\n",
    "        collections=[collection],\n",
    "        bbox=bounds,\n",
    "        datetime=time_of_interest,\n",
    "        query={\"eo:cloud_cover\": {\"lt\": cloud_threshold}},\n",
    "    ).item_collection()\n",
    "    \n",
    "    if len(items) == 0:\n",
    "        if verbose:\n",
    "            print(f\"  [{year_tag}] No items found\")\n",
    "        return None\n",
    "    \n",
    "    if items_per_group > 0:\n",
    "        sampled_items = stratified_sample_items(items, items_per_group, max_items)\n",
    "        if verbose:\n",
    "            print(f\"  [{year_tag}] {len(sampled_items)} items from {len(items)} total\")\n",
    "    else:\n",
    "        items_sorted = sorted(items, key=lambda x: x.properties.get('eo:cloud_cover', 100))\n",
    "        sampled_items = items_sorted[:max_items]\n",
    "    \n",
    "    try:\n",
    "        bands = [red_band, nir_band, blue_band] if evi else [red_band, nir_band]\n",
    "        t0 = time.time()\n",
    "        \n",
    "        # Use chunks for parallel S3 fetching via dask\n",
    "        ds = odc.stac.load(\n",
    "            sampled_items,\n",
    "            crs=\"EPSG:3857\",\n",
    "            bands=bands,\n",
    "            resolution=pixel_spacing,\n",
    "            bbox=bounds,\n",
    "            chunks={'x': 2048, 'y': 2048, 'time': 1},  # Parallel chunk loading\n",
    "            fail_on_error=False,\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"  [{year_tag}] Load graph built: {time.time() - t0:.1f}s\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        if verbose:\n",
    "            print(f\"  [{year_tag}] Error: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if not ds.data_vars:\n",
    "        return None\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Build lazy computation graph - float32 for memory efficiency\n",
    "    ds = ds.astype('float32') * 0.0000275 - 0.2\n",
    "    \n",
    "    # Calculate vegetation index (still lazy)\n",
    "    if evi:\n",
    "        import xarray as xr\n",
    "        nir = ds[nir_band]\n",
    "        red = ds[red_band]\n",
    "        blue = ds[blue_band]\n",
    "        denominator = nir + 6 * red - 7.5 * blue + 1\n",
    "        vi_arr = xr.where(\n",
    "            denominator > 1.0,\n",
    "            2.5 * ((nir - red) / denominator),\n",
    "            np.float32(np.nan)\n",
    "        )\n",
    "    else:\n",
    "        vi_arr = (ds[nir_band] - ds[red_band]) / (ds[nir_band] + ds[red_band])\n",
    "        vi_arr = vi_arr.clip(min=-1.0, max=1.0)\n",
    "    \n",
    "    # Max across time - SINGLE .compute() triggers all I/O + compute in parallel\n",
    "    arr = vi_arr.max(dim=\"time\", skipna=True).compute()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  [{year_tag}] Fetch+VI+max: {time.time() - t0:.1f}s\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Extract coordinates once\n",
    "    x_coords = arr.coords['x'].values\n",
    "    y_coords = arr.coords['y'].values\n",
    "    ny, nx = len(y_coords), len(x_coords)\n",
    "    \n",
    "    # Build coordinate arrays - use broadcasting to avoid meshgrid memory copy\n",
    "    xx = np.tile(x_coords, ny)\n",
    "    yy = np.repeat(y_coords, nx)\n",
    "    values = arr.values.ravel()\n",
    "    \n",
    "    # Filter invalid values BEFORE reprojection (fewer coords to transform)\n",
    "    mask = np.isfinite(values)\n",
    "    xx_valid = xx[mask]\n",
    "    yy_valid = yy[mask]\n",
    "    values_valid = values[mask].astype('float64')\n",
    "    \n",
    "    del arr, xx, yy, values  # Free memory\n",
    "    \n",
    "    # Reproject only valid points\n",
    "    transformer = Transformer.from_crs(\"EPSG:3857\", \"EPSG:4326\", always_xy=True)\n",
    "    lons, lats = transformer.transform(xx_valid, yy_valid)\n",
    "    \n",
    "    del xx_valid, yy_valid\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  [{year_tag}] Reproject {len(lats):,} pts: {time.time() - t0:.1f}s\")\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    # Create Arrow table\n",
    "    py_table = pa.table({\n",
    "        'lat': pa.array(lats, type=pa.float64()),\n",
    "        'lon': pa.array(lons, type=pa.float64()),\n",
    "        'data': pa.array(values_valid, type=pa.float64())\n",
    "    })\n",
    "    \n",
    "    del lats, lons, values_valid\n",
    "    \n",
    "    # H3 aggregation\n",
    "    con = get_con()\n",
    "    agg_map = {'mean': 'AVG', 'sum': 'SUM', 'max': 'MAX', 'min': 'MIN', 'median': 'MEDIAN'}\n",
    "    sql_agg = agg_map.get(agg_func, 'AVG')\n",
    "    \n",
    "    h3_result = con.execute(f\"\"\"\n",
    "        SELECT \n",
    "            h3_latlng_to_cell_string(lat, lon, {h3_res}) AS hex,\n",
    "            {sql_agg}(data)::DOUBLE AS metric\n",
    "        FROM py_table\n",
    "        WHERE data IS NOT NULL AND isfinite(data)\n",
    "        GROUP BY 1\n",
    "    \"\"\").fetch_arrow_table()\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"  [{year_tag}] H3 agg: {time.time() - t0:.1f}s → {h3_result.num_rows:,} cells\")\n",
    "    \n",
    "    return h3_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_h3_results(h3_tables: List[pa.Table], agg_func: str = \"mean\") -> pa.Table:\n",
    "    \"\"\"\n",
    "    Combine multiple H3 aggregation results into one.\n",
    "    \n",
    "    Re-aggregates overlapping hexagons across time periods.\n",
    "    \"\"\"\n",
    "    if not h3_tables:\n",
    "        return None\n",
    "    \n",
    "    combined = pa.concat_tables(h3_tables)\n",
    "    \n",
    "    con = get_con()\n",
    "    agg_map = {'mean': 'AVG', 'sum': 'SUM', 'max': 'MAX', 'min': 'MIN', 'median': 'MEDIAN'}\n",
    "    sql_agg = agg_map.get(agg_func, 'AVG')\n",
    "    \n",
    "    return con.execute(f\"\"\"\n",
    "        SELECT hex, {sql_agg}(metric)::DOUBLE AS metric\n",
    "        FROM combined\n",
    "        GROUP BY hex\n",
    "    \"\"\").fetch_arrow_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_period_diff(h3_first: pa.Table, h3_second: pa.Table):\n",
    "    \"\"\"\n",
    "    Compute difference between two H3 aggregated periods.\n",
    "    \n",
    "    Arrow in, pandas out (final result is small).\n",
    "    \"\"\"\n",
    "    con = get_con()\n",
    "    \n",
    "    return con.sql(\"\"\"\n",
    "        SELECT \n",
    "            h3_first.hex,\n",
    "            ROUND((h3_second.metric - h3_first.metric) * 100, 3) AS pct_change_evi,\n",
    "            ROUND(h3_first.metric::DOUBLE, 3) AS early_avg,\n",
    "            ROUND(h3_second.metric::DOUBLE, 3) AS current_avg\n",
    "        FROM h3_first \n",
    "        INNER JOIN h3_second ON h3_first.hex = h3_second.hex\n",
    "    \"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(fn, arg_list, max_workers: int = 24):\n",
    "    \"\"\"Execute function in parallel with ThreadPoolExecutor.\"\"\"\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as pool:\n",
    "        return list(pool.map(fn, arg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_years_streaming(\n",
    "    bounds: List[float],\n",
    "    years: List[int],\n",
    "    h3_res: int,\n",
    "    evi: bool = True,\n",
    "    cloud_threshold: int = 30,\n",
    "    max_items: int = 50,\n",
    "    items_per_group: int = 2,\n",
    "    agg_func: str = \"mean\",\n",
    "    verbose: bool = False\n",
    ") -> Optional[pa.Table]:\n",
    "    \"\"\"\n",
    "    Process multiple years with concurrency = len(years).\n",
    "    \n",
    "    All years in a period run in parallel, but periods are sequential.\n",
    "    \"\"\"\n",
    "    time_periods = [f\"{year}-04-20/{year}-11-10\" for year in years]\n",
    "    concurrency = len(time_periods)\n",
    "    \n",
    "    h3_results = []\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=concurrency) as pool:\n",
    "        futures = {}\n",
    "        for period in time_periods:\n",
    "            future = pool.submit(\n",
    "                process_single_period_to_h3,\n",
    "                time_of_interest=period,\n",
    "                bounds=bounds,\n",
    "                h3_res=h3_res,\n",
    "                evi=evi,\n",
    "                cloud_threshold=cloud_threshold,\n",
    "                max_items=max_items,\n",
    "                items_per_group=items_per_group,\n",
    "                agg_func=agg_func,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            futures[future] = period\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                h3_results.append(result)\n",
    "    \n",
    "    if not h3_results:\n",
    "        return None\n",
    "    \n",
    "    return combine_h3_results(h3_results, agg_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_diff(\n",
    "    bounds: List[float],\n",
    "    h3_res: int = 7,\n",
    "    first_years: List[int] = [1992, 1993, 1994],\n",
    "    second_years: List[int] = [2021, 2022, 2023],\n",
    "    cloud_threshold: int = 30,\n",
    "    evi: bool = True,\n",
    "    agg_func: str = \"mean\",\n",
    "    max_items: int = 50,\n",
    "    items_per_group: int = 2,\n",
    "    max_concurrent_years: int = 3,  # Limit concurrency since dask handles parallelism within each year\n",
    "    verbose: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute vegetation index change between two time periods.\n",
    "    \n",
    "    Uses dask for parallel I/O within each year, with controlled year-level concurrency.\n",
    "    \"\"\"\n",
    "    all_years = first_years + second_years\n",
    "    time_periods = [f\"{year}-04-20/{year}-11-10\" for year in all_years]\n",
    "    \n",
    "    # Limit concurrent years to avoid oversubscription (dask handles parallelism within each)\n",
    "    num_workers = min(max_concurrent_years, len(time_periods))\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Processing {len(all_years)} years ({num_workers} concurrent)\\n\")\n",
    "    \n",
    "    h3_results = {}\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as pool:\n",
    "        futures = {}\n",
    "        for year, period in zip(all_years, time_periods):\n",
    "            future = pool.submit(\n",
    "                process_single_period_to_h3,\n",
    "                time_of_interest=period,\n",
    "                bounds=bounds,\n",
    "                h3_res=h3_res,\n",
    "                evi=evi,\n",
    "                cloud_threshold=cloud_threshold,\n",
    "                max_items=max_items,\n",
    "                items_per_group=items_per_group,\n",
    "                agg_func=agg_func,\n",
    "                verbose=verbose\n",
    "            )\n",
    "            futures[future] = year\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            year = futures[future]\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                h3_results[year] = result\n",
    "    \n",
    "    # Split results back into first/second periods\n",
    "    first_tables = [h3_results[y] for y in first_years if y in h3_results]\n",
    "    second_tables = [h3_results[y] for y in second_years if y in h3_results]\n",
    "    \n",
    "    if not first_tables or not second_tables:\n",
    "        if verbose:\n",
    "            print(\"Missing data for one or both periods\")\n",
    "        return None\n",
    "    \n",
    "    h3_first = combine_h3_results(first_tables, agg_func)\n",
    "    h3_second = combine_h3_results(second_tables, agg_func)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nEarly period: {h3_first.num_rows:,} H3 cells\")\n",
    "        print(f\"Recent period: {h3_second.num_rows:,} H3 cells\")\n",
    "        print(\"Computing difference...\")\n",
    "    \n",
    "    result = compute_period_diff(h3_first, h3_second)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Final: {len(result):,} matched hexagons\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df, filename: str):\n",
    "    \"\"\"Save DataFrame to parquet.\"\"\"\n",
    "    qr=f\"copy (from df) to '{filename}' (FORMAT parquet, PARQUET_VERSION v2)\"\n",
    "    duckdb.sql(qr)\n",
    "    print(f\"Saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "Get your bounding box from [boundingbox.klokantech.com](https://boundingbox.klokantech.com/) — select **CSV** format and paste directly as `bounds = [...]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pripyat/Chernobyl exclusion zone\n",
    "BOUNDS = [29.1026, 51.045, 30.6678, 51.8343]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run analysis\n",
    "df = get_h3_diff(\n",
    "    bounds=BOUNDS,\n",
    "    h3_res=8,\n",
    "    first_years=[1992, 1993, 1994],\n",
    "    second_years=[2022, 2023, 2024],\n",
    "    evi=True,\n",
    "    max_items=50,\n",
    "    items_per_group=2,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "if df is not None:\n",
    "    print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to pripyat_EVI_change_v2_res_8.parquet\n"
     ]
    }
   ],
   "source": [
    "# Optionally save results\n",
    "save_to_parquet(df, 'pripyat_EVI_change_v2_res_8.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =duckdb.sql(\"from read_parquet('pripyat_EVI_change_v2_res_8.parquet')\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def visualize_h3_diff(\n    df,\n    mapbox_token: str,\n    column: str = 'pct_change_evi',\n    opacity: float = 0.3592,\n    coverage: float = 1\n):\n    \"\"\"\n    Visualize H3 difference data with lonboard.\n    \n    Returns (map, h3_layer) so you can mutate colors without reloading.\n    \"\"\"\n    from lonboard import Map, H3HexagonLayer, BitmapTileLayer\n    from lonboard.colormap import apply_continuous_cmap\n    from matplotlib.colors import TwoSlopeNorm, Normalize\n    from palettable.scientific.diverging import Roma_20\n    \n    # Use TwoSlopeNorm for change columns (center at 0), linear for averages\n    if 'change' in column:\n        norm = TwoSlopeNorm(\n            vmin=df[column].quantile(0.05),\n            vcenter=0,\n            vmax=df[column].quantile(0.95)\n        )\n    else:\n        norm = Normalize(\n            vmin=df[column].quantile(0.05),\n            vmax=df[column].quantile(0.95)\n        )\n    \n    normalized = norm(df[column])\n    colors = apply_continuous_cmap(normalized, Roma_20, alpha=1)\n    \n    h3_layer = H3HexagonLayer.from_pandas(\n        df,\n        get_hexagon=df[\"hex\"],\n        get_fill_color=colors,\n        high_precision=True,\n        auto_highlight=True,\n        extruded=False,\n        stroked=False,\n        coverage=coverage,\n        opacity=opacity,\n    )\n    \n    basemap = BitmapTileLayer(\n        data=f\"https://api.mapbox.com/styles/v1/mapbox/satellite-streets-v12/tiles/512/{{z}}/{{x}}/{{y}}@2x?access_token={mapbox_token}\",\n        tile_size=512,\n        max_requests=-1,\n    )\n    \n    return Map(layers=[basemap, h3_layer]), h3_layer\n\n\ndef update_colors(h3_layer, df, column: str = 'pct_change_evi'):\n    \"\"\"Update layer colors without reloading data.\"\"\"\n    from lonboard.colormap import apply_continuous_cmap\n    from matplotlib.colors import TwoSlopeNorm, Normalize\n    from palettable.scientific.diverging import Roma_20\n    \n    if 'change' in column:\n        norm = TwoSlopeNorm(\n            vmin=df[column].quantile(0.05),\n            vcenter=0,\n            vmax=df[column].quantile(0.95)\n        )\n    else:\n        norm = Normalize(\n            vmin=df[column].quantile(0.05),\n            vmax=df[column].quantile(0.95)\n        )\n    \n    normalized = norm(df[column])\n    h3_layer.get_fill_color = apply_continuous_cmap(normalized, Roma_20, alpha=1)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create map (loads data once)\nload_dotenv()\nMAPBOX_TOKEN = os.environ.get('MAPBOX_TOKEN')\nm, h3_layer = visualize_h3_diff(df, MAPBOX_TOKEN)\nm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from palettable.scientific.diverging import Roma_15\n",
    "Roma_15.mpl_colormap"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Switch to 90s average (no reload, just updates colors)\nupdate_colors(h3_layer, df, 'early_avg')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Switch to 2020s average\nupdate_colors(h3_layer, df, 'current_avg')"
  },
  {
   "cell_type": "code",
   "source": "# Switch back to change\nupdate_colors(h3_layer, df, 'pct_change_evi')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "landsat-diff-h3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}